{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94548a0",
   "metadata": {},
   "source": [
    "* TOC\n",
    "{:toc}\n",
    "\n",
    "## 第二十四章：Python与数据科学 \n",
    "\n",
    "### 第三节：Python中常用的机器学习库 \n",
    "\n",
    "Python 是数据科学和机器学习领域中最受欢迎的编程语言之一。其丰富的库生态系统为机器学习提供了强大的支持。以下是一些常见的 Python 机器学习库及其详细介绍：\n",
    "\n",
    "#### 1. Scikit-learn \n",
    "\n",
    "简介：Scikit-learn 是一个简单而高效的工具，用于数据挖掘和数据分析，建立在 NumPy、SciPy 和 matplotlib 之上。它提供了一系列常用的机器学习算法和工具。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  分类：支持多种分类算法，如 SVM、k-NN、随机森林等。\n",
    " *  回归：提供线性回归、岭回归、Lasso 回归等模型。\n",
    " *  聚类：支持 k-means、层次聚类、DBSCAN 等聚类算法。\n",
    " *  降维：包括 PCA、LDA 等降维技术。\n",
    " *  模型选择：提供交叉验证、网格搜索等模型选择工具。\n",
    "\n",
    "#### 2. TensorFlow \n",
    "\n",
    "简介：TensorFlow 是由 Google 开发的一个开源机器学习框架，广泛用于深度学习研究和应用。它支持从研究原型到生产部署的整个过程。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  灵活的模型构建：支持静态图和动态图模式，适应不同的开发需求。\n",
    " *  高效的计算：利用 GPU 和 TPU 加速计算，适合大规模数据处理。\n",
    " *  丰富的生态系统：包括 TensorFlow Lite、TensorFlow.js 等，支持移动端和浏览器端的模型部署。\n",
    " *  Keras：内置 Keras 高级 API，简化深度学习模型的构建和训练。\n",
    "\n",
    "#### 3. PyTorch \n",
    "\n",
    "简介：PyTorch 是由 Facebook 开发的一个开源深度学习框架，以其灵活性和易用性受到广泛欢迎。它特别适合研究和原型开发。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  动态图计算：支持动态计算图，使调试和开发更加直观。\n",
    " *  自动微分：内置自动微分功能，简化梯度计算。\n",
    " *  丰富的社区资源：拥有大量的预训练模型和教程，方便用户快速上手。\n",
    " *  TorchScript：支持将模型转换为静态图，以便在生产环境中高效运行。\n",
    "\n",
    "#### 4. Keras \n",
    "\n",
    "简介：Keras 是一个高层神经网络 API，由 Python 编写，并能够运行在 TensorFlow、Theano 和 CNTK 之上。它的设计理念是用户友好和快速实验。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  简洁的 API：提供简洁易用的接口，适合快速构建和训练深度学习模型。\n",
    " *  模块化：支持模块化构建，用户可以自由组合不同的层、优化器和损失函数。\n",
    " *  多后端支持：可以在多种深度学习框架上运行，提供灵活性。\n",
    "\n",
    "#### 5. XGBoost \n",
    "\n",
    "简介：XGBoost 是一个优化的分布式梯度提升库，具有高效、灵活和可移植的特点，被广泛应用于 Kaggle 竞赛和工业界。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  高效的提升算法：实现了高效的梯度提升算法，支持并行计算。\n",
    " *  灵活的模型：支持多种目标函数和评估指标，适应不同的任务需求。\n",
    " *  跨平台支持：可以在多种操作系统和分布式环境下运行。\n",
    "\n",
    "#### 6. LightGBM \n",
    "\n",
    "简介：LightGBM 是由微软开发的一个高效的梯度提升框架，专为速度和效率优化，适合处理大规模数据集。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  高效的训练：采用基于直方图的算法，大大提高了训练速度。\n",
    " *  支持大规模数据：能够处理数亿条数据和数万维特征。\n",
    " *  多种并行策略：支持数据并行、特征并行和混合并行，提高计算效率。\n",
    "\n",
    "#### 7. Pandas \n",
    "\n",
    "简介：Pandas 是一个强大的数据处理和分析库，提供了数据结构和数据分析工具，特别适合处理结构化数据。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  数据结构：提供了 DataFrame 和 Series 数据结构，方便数据操作。\n",
    " *  数据清洗：支持缺失值处理、数据转换和数据合并等操作。\n",
    " *  数据分析：内置丰富的统计分析函数，方便进行数据分析和探索。\n",
    "\n",
    "#### 8. NumPy \n",
    "\n",
    "简介：NumPy 是一个基础的科学计算库，提供了多维数组对象和各种数学函数，被广泛用于数值计算和数据处理。\n",
    "\n",
    "主要功能：\n",
    "\n",
    " *  多维数组：提供高效的多维数组对象，支持各种数组操作。\n",
    " *  数学函数：内置丰富的数学函数库，支持线性代数、傅里叶变换等操作。\n",
    " *  随机数生成：提供随机数生成器，支持各种概率分布。\n",
    "\n",
    "#### 总结 \n",
    "\n",
    "Python 的机器学习库生态系统非常丰富，每个库都有其独特的优势和应用场景。选择合适的库能够显著提高开发效率和模型性能。希望以上介绍能够帮助你更好地了解和使用这些强大的工具。\n",
    "\n",
    "#### python中与常用的机器学习库相关的面试笔试题 \n",
    "\n",
    "在该节中，我们只是初步讲解，后续会单独开章节，专门介绍python中与机器学习相关的面试笔试题。\n",
    "\n",
    "#### 面试题 1 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 Scikit-learn 实现一个简单的线性回归模型，并使用该模型预测给定数据集的目标值。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  Scikit-learn 基本使用：是否熟悉 Scikit-learn 库的基本使用方法，包括导入数据、构建模型、训练模型和预测。\n",
    "2.  线性回归：是否理解线性回归的基本概念和应用场景。\n",
    "3.  数据处理：是否具备基本的数据处理能力，包括数据拆分和标准化等。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea14115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 生成一个简单的数据集\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# 将数据集拆分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建线性回归模型\n",
    "model = LinearRegression()\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 使用模型进行预测\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 计算模型的均方误差\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# 输出模型的系数和截距\n",
    "print(f\"Coefficient: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d298d3",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 NumPy、Pandas、Scikit-learn 的模型选择模块、线性回归模块和评估模块。\n",
    "2.  生成数据集：使用 NumPy 生成一个简单的线性数据集，其中 `X` 是自变量，`y` 是因变量，带有随机噪声。\n",
    "3.  数据拆分：使用 `train_test_split` 函数将数据集拆分为训练集和测试集，测试集占 20%。\n",
    "4.  创建模型：创建一个线性回归模型实例。\n",
    "5.  训练模型：使用训练集数据训练模型。\n",
    "6.  预测：使用训练好的模型对测试集数据进行预测。\n",
    "7.  评估模型：计算模型的均方误差（MSE）以评估模型性能。\n",
    "8.  输出结果：输出模型的系数和截距。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 Scikit-learn 的基本使用、线性回归的理解以及数据处理能力。\n",
    "\n",
    "#### 面试题 2 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 Scikit-learn 实现一个简单的 K-means 聚类模型，并对给定数据集进行聚类分析，展示聚类结果。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  Scikit-learn 基本使用：是否熟悉 Scikit-learn 库的基本使用方法，包括数据导入、模型构建、训练和预测。\n",
    "2.  K-means 聚类：是否理解 K-means 聚类的基本概念和应用场景。\n",
    "3.  数据处理和可视化：是否具备基本的数据处理和结果可视化能力。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 生成一个简单的二维数据集\n",
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# 创建 K-means 模型，指定聚类中心数量为 4\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# 训练模型\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 预测每个样本的簇标签\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "# 可视化聚类结果\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "# 画出聚类中心\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')\n",
    "plt.title('K-means Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7bc70",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 NumPy、Matplotlib 和 Scikit-learn 的数据集生成模块和聚类模块。\n",
    "2.  生成数据集：使用 `make_blobs` 函数生成一个简单的二维数据集，其中 `X` 是特征，`y` 是簇标签（用于验证）。\n",
    "3.  创建模型：创建一个 K-means 聚类模型实例，并指定聚类中心的数量为 4。\n",
    "4.  训练模型：使用生成的数据集训练 K-means 模型。\n",
    "5.  预测：使用训练好的模型对数据集进行聚类预测，得到每个样本的簇标签。\n",
    "6.  可视化结果：使用 Matplotlib 可视化聚类结果，展示每个样本的簇标签，并用红色标记聚类中心。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 Scikit-learn 的基本使用、K-means 聚类算法的理解以及数据处理和可视化能力。\n",
    "\n",
    "#### 面试题 3 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 TensorFlow 实现一个简单的卷积神经网络（CNN），用于对 MNIST 手写数字数据集进行分类，并报告模型在测试集上的准确率。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  TensorFlow 基本使用：是否熟悉 TensorFlow 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  卷积神经网络：是否理解卷积神经网络的基本概念和应用场景。\n",
    "3.  数据处理：是否具备基本的数据预处理能力，包括数据标准化和批处理等。\n",
    "4.  模型评估：是否能够正确评估模型性能。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b1386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 加载 MNIST 数据集\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# 数据预处理\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# 构建卷积神经网络模型\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# 添加分类器\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa9861",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 TensorFlow 和 Keras 模块。\n",
    "2.  加载数据集：使用 TensorFlow 的 Keras 模块加载 MNIST 手写数字数据集。\n",
    "3.  数据预处理：将训练集和测试集的数据形状调整为 (28, 28, 1)，并将像素值标准化到 \\[0, 1\\] 范围。将标签转换为 one-hot 编码。\n",
    "4.  构建模型：使用 Keras 的 Sequential API 构建卷积神经网络模型。模型包括三个卷积层和两个最大池化层，最后添加一个全连接层和一个输出层。\n",
    "5.  编译模型：使用 Adam 优化器和 categorical\\_crossentropy 损失函数编译模型，并指定评估指标为准确率。\n",
    "6.  训练模型：使用训练数据训练模型，设置训练轮数为 5，批次大小为 64，并使用 10% 的训练数据进行验证。\n",
    "7.  评估模型：使用测试数据评估模型性能，并输出测试集上的准确率。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 TensorFlow 的基本使用、卷积神经网络的理解以及数据处理和模型评估能力。\n",
    "\n",
    "#### 面试题 4 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 TensorFlow 实现一个简单的循环神经网络（RNN），用于对 IMDB 电影评论数据集进行情感分类，并报告模型在测试集上的准确率。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  TensorFlow 基本使用：是否熟悉 TensorFlow 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  循环神经网络：是否理解循环神经网络的基本概念和应用场景。\n",
    "3.  文本数据处理：是否具备基本的文本数据预处理能力，包括文本向量化和填充等。\n",
    "4.  模型评估：是否能够正确评估模型性能。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# 加载 IMDB 数据集\n",
    "max_features = 10000  # 只考虑最常见的 10000 个单词\n",
    "maxlen = 500  # 每个评论只考虑前 500 个单词\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 数据预处理：填充序列\n",
    "train_data = sequence.pad_sequences(train_data, maxlen=maxlen)\n",
    "test_data = sequence.pad_sequences(test_data, maxlen=maxlen)\n",
    "\n",
    "# 构建循环神经网络模型\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(layers.SimpleRNN(64, return_sequences=False))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 训练模型\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# 评估模型\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e3b67",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 TensorFlow 和 Keras 模块。\n",
    "2.  加载数据集：使用 TensorFlow 的 Keras 模块加载 IMDB 电影评论数据集，并限制只考虑最常见的 10000 个单词。\n",
    "3.  数据预处理：将训练集和测试集的评论序列填充或截断到固定长度 500，以便输入到 RNN 模型。\n",
    "4.  构建模型：使用 Keras 的 Sequential API 构建循环神经网络模型。模型包括一个嵌入层（将单词索引转换为密集向量）、一个简单的 RNN 层和一个输出层。\n",
    "5.  编译模型：使用 Adam 优化器和二元交叉熵损失函数编译模型，并指定评估指标为准确率。\n",
    "6.  训练模型：使用训练数据训练模型，设置训练轮数为 5，批次大小为 64，并使用 20% 的训练数据进行验证。\n",
    "7.  评估模型：使用测试数据评估模型性能，并输出测试集上的准确率。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 TensorFlow 的基本使用、循环神经网络的理解以及文本数据处理和模型评估能力。\n",
    "\n",
    "#### 面试题 5 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 PyTorch 实现一个简单的前馈神经网络（Feedforward Neural Network），用于对 CIFAR-10 图像数据集进行分类，并报告模型在测试集上的准确率。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  PyTorch 基本使用：是否熟悉 PyTorch 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  前馈神经网络：是否理解前馈神经网络的基本概念和应用场景。\n",
    "3.  数据处理：是否具备基本的数据预处理能力，包括数据标准化和批处理等。\n",
    "4.  模型评估：是否能够正确评估模型性能。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90f1e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# 加载 CIFAR-10 数据集\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# 定义前馈神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 3 * 32 * 32)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(5):  # 训练 5 个 epoch\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # 每 100 个 mini-batch 打印一次损失\n",
    "            print(f'[Epoch {epoch + 1}, Mini-batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# 在测试集上评估模型\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da1076",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 PyTorch 和 Torchvision 模块。\n",
    "2.  数据预处理：定义数据预处理步骤，包括将图像转换为张量并归一化。\n",
    "3.  加载数据集：使用 Torchvision 加载 CIFAR-10 图像数据集，并创建数据加载器进行批处理。\n",
    "4.  定义模型：定义一个简单的前馈神经网络模型，包括三个全连接层。使用 ReLU 激活函数。\n",
    "5.  定义损失函数和优化器：使用交叉熵损失函数和 Adam 优化器。\n",
    "6.  训练模型：使用训练数据训练模型，设置训练轮数为 5，每个 mini-batch 的大小为 64，并在每 100 个 mini-batch 后打印一次损失。\n",
    "7.  评估模型：使用测试数据评估模型性能，并输出测试集上的准确率。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 PyTorch 的基本使用、前馈神经网络的理解以及数据处理和模型评估能力。\n",
    "\n",
    "#### 面试题 6 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 XGBoost 实现一个分类模型，用于对 UCI 的 Adult 数据集进行收入分类，并报告模型在测试集上的准确率。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  XGBoost 基本使用：是否熟悉 XGBoost 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  特征工程：是否具备基本的特征工程能力，包括数据预处理、特征选择和特征转换等。\n",
    "3.  模型评估：是否能够正确评估模型性能。\n",
    "4.  分类问题：是否理解分类问题的基本概念和应用场景。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据集\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', \n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', \n",
    "    'hours_per_week', 'native_country', 'income'\n",
    "]\n",
    "data = pd.read_csv(url, names=columns, na_values=' ?', skipinitialspace=True)\n",
    "\n",
    "# 数据预处理\n",
    "data.dropna(inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# 分割数据集\n",
    "X = data.drop('income', axis=1)\n",
    "y = data['income']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 设置参数\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'eval_metric': 'error'\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(dtest)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# 评估模型\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae6c01",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 pandas、xgboost 和 scikit-learn 模块。\n",
    "2.  加载数据集：使用 pandas 加载 UCI 的 Adult 数据集，并指定列名。\n",
    "3.  数据预处理：处理缺失值并将分类特征转换为数值编码。使用 scikit-learn 的 LabelEncoder 进行编码。\n",
    "4.  分割数据集：将数据集分割为训练集和测试集，测试集占总数据的 20%。\n",
    "5.  转换为 DMatrix 格式：将数据转换为 XGBoost 的 DMatrix 格式，以提高计算效率。\n",
    "6.  设置参数：定义 XGBoost 模型的参数，包括目标函数、最大深度、学习率和评估指标。\n",
    "7.  训练模型：使用训练数据训练 XGBoost 模型，设置迭代次数为 100。\n",
    "8.  预测：使用测试数据进行预测，并将预测结果转换为二元分类结果。\n",
    "9.  评估模型：使用 scikit-learn 的 accuracy\\_score 评估模型在测试集上的准确率，并输出结果。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 XGBoost 的基本使用、特征工程的理解以及数据处理和模型评估能力。\n",
    "\n",
    "#### 面试题 7 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 XGBoost 实现一个回归模型，用于预测加州房价数据集中的房价，并报告模型在测试集上的均方误差（MSE）。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  XGBoost 基本使用：是否熟悉 XGBoost 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  特征工程：是否具备基本的特征工程能力，包括数据预处理、特征选择和特征转换等。\n",
    "3.  模型评估：是否能够正确评估模型性能。\n",
    "4.  回归问题：是否理解回归问题的基本概念和应用场景。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394cbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 加载加州房价数据集\n",
    "california_housing = fetch_california_housing()\n",
    "X, y = california_housing.data, california_housing.target\n",
    "\n",
    "# 分割数据集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 DMatrix 格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 设置参数\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.3,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "bst = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# 评估模型\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2c6c39",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 pandas、xgboost 和 scikit-learn 模块。\n",
    "2.  加载数据集：使用 scikit-learn 加载加州房价数据集，并获取特征矩阵 X 和目标向量 y。\n",
    "3.  分割数据集：将数据集分割为训练集和测试集，测试集占总数据的 20%。\n",
    "4.  转换为 DMatrix 格式：将数据转换为 XGBoost 的 DMatrix 格式，以提高计算效率。\n",
    "5.  设置参数：定义 XGBoost 模型的参数，包括目标函数、最大深度、学习率和评估指标。\n",
    "6.  训练模型：使用训练数据训练 XGBoost 模型，设置迭代次数为 100。\n",
    "7.  预测：使用测试数据进行预测。\n",
    "8.  评估模型：使用 scikit-learn 的 mean\\_squared\\_error 评估模型在测试集上的均方误差，并输出结果。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 XGBoost 的基本使用、特征工程的理解以及数据处理和模型评估能力。\n",
    "\n",
    "#### 面试题 8 \n",
    "\n",
    "##### 面试题目 \n",
    "\n",
    "请使用 LightGBM 实现一个分类模型，用于对 Kaggle 的 Titanic 数据集进行生存预测，并报告模型在测试集上的准确率。\n",
    "\n",
    "##### 考点解析 \n",
    "\n",
    "该问题主要考察以下几个方面：\n",
    "\n",
    "1.  LightGBM 基本使用：是否熟悉 LightGBM 库的基本使用方法，包括数据导入、模型构建、训练和评估。\n",
    "2.  特征工程：是否具备基本的特征工程能力，包括数据预处理、特征选择和特征转换等。\n",
    "3.  模型评估：是否能够正确评估模型性能。\n",
    "4.  分类问题：是否理解分类问题的基本概念和应用场景。\n",
    "\n",
    "##### 答案或代码 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6646f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据集\n",
    "url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# 数据预处理\n",
    "data.drop(['Name', 'Ticket', 'Cabin'], axis=1, inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "label_encoder = LabelEncoder()\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# 分割数据集\n",
    "X = data.drop('Survived', axis=1)\n",
    "y = data['Survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为 LightGBM 格式\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dtest = lgb.Dataset(X_test, label=y_test, reference=dtrain)\n",
    "\n",
    "# 设置参数\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "\n",
    "# 训练模型\n",
    "bst = lgb.train(params, dtrain, num_boost_round=100, valid_sets=dtest, early_stopping_rounds=10)\n",
    "\n",
    "# 预测\n",
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# 评估模型\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfaaa2",
   "metadata": {},
   "source": [
    "\n",
    "##### 答案或代码解析 \n",
    "\n",
    "1.  导入库：首先导入必要的库，包括 pandas、lightgbm 和 scikit-learn 模块。\n",
    "2.  加载数据集：使用 pandas 加载 Kaggle 的 Titanic 数据集，并删除无关特征（Name, Ticket, Cabin）。\n",
    "3.  数据预处理：处理缺失值并将分类特征转换为数值编码。使用 scikit-learn 的 LabelEncoder 进行编码。\n",
    "4.  分割数据集：将数据集分割为训练集和测试集，测试集占总数据的 20%。\n",
    "5.  转换为 LightGBM 格式：将数据转换为 LightGBM 的 Dataset 格式，以提高计算效率。\n",
    "6.  设置参数：定义 LightGBM 模型的参数，包括目标函数、评估指标、提升类型、叶子节点数、学习率和特征选择比例。\n",
    "7.  训练模型：使用训练数据训练 LightGBM 模型，设置迭代次数为 100，并在验证集上进行早停。\n",
    "8.  预测：使用测试数据进行预测，并将预测结果转换为二元分类结果。\n",
    "9.  评估模型：使用 scikit-learn 的 accuracy\\_score 评估模型在测试集上的准确率，并输出结果。\n",
    "\n",
    "通过这个例子，面试官可以评估候选人对 LightGBM 的基本使用、特征工程的理解以及数据处理和模型评估能力。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
